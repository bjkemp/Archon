# Base URL for the LLM instance
# OpenAI: https://api.openai.com/v1
# Ollama (example): http://localhost:11434/v1
# OpenRouter: https://openrouter.ai/api/v1
# Anthropic: https://api.anthropic.com/v1
# Gemini: No base URL needed (handled internally)
BASE_URL=

# LLM Provider Selection
# Valid options: OpenAI, Anthropic, OpenRouter, Ollama, Gemini
LLM_PROVIDER=OpenAI

# LLM API Keys
# For OpenAI: https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# For Anthropic: https://console.anthropic.com/account/keys
# For OpenRouter: https://openrouter.ai/keys
# For Ollama, no need to set this unless you specifically configured an API key
# For Gemini: https://ai.google.dev/tutorials/setup
LLM_API_KEY=

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# Even if using Anthropic, OpenRouter, or Gemini, you still need to set this for the embedding model.
# No need to set this if using Ollama.
OPENAI_API_KEY=

# Google API Key for Gemini models
# Only needed if LLM_PROVIDER is set to "Gemini"
# Get your API key from: https://ai.google.dev/tutorials/setup
GOOGLE_API_KEY=

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# The LLM you want to use for the reasoner (o3-mini, R1, QwQ, etc.).
# Example for OpenAI: gpt-4o-mini
# Example for Anthropic: claude-3-5-sonnet
# Example for Ollama: llama3
# Example for Gemini: gemini-2.5-pro
REASONER_MODEL=

# The LLM you want to use for the primary agent/coder.
# Example for OpenAI: gpt-4o
# Example for Anthropic: claude-3-opus
# Example for Ollama: llama3:latest
# Example for Gemini: gemini-2.5-pro
PRIMARY_MODEL=

# Embedding model you want to use
# Example for Ollama: nomic-embed-text
# Example for OpenAI: text-embedding-3-small
EMBEDDING_MODEL=

# Advanced model parameters (optional)
# Temperature for generation (0.0 to 1.0, lower is more deterministic)
MODEL_TEMPERATURE=0.7

# Top-p sampling (0.0 to 1.0)
MODEL_TOP_P=0.95

# Maximum tokens to generate in responses
MODEL_MAX_TOKENS=4096

# Fallback order for models in case of API failures
# Comma-separated list of providers: OpenAI,Anthropic,Ollama,Gemini
MODEL_FALLBACK_ORDER=
