--- streamlit_pages/environment.py.orig
+++ streamlit_pages/environment.py
@@ -102,7 +102,8 @@
         "OpenAI": "https://api.openai.com/v1",
         "Anthropic": "https://api.anthropic.com/v1",
         "OpenRouter": "https://openrouter.ai/api/v1",
-        "Ollama": "http://localhost:11434/v1"
+        "Ollama": "http://localhost:11434/v1",
+        "Gemini": "N/A"  # Gemini uses the Google API directly
     }
     
     embedding_default_urls = {
@@ -125,7 +126,7 @@
     st.subheader("1. Select Your LLM Provider")
     
     # LLM Provider dropdown
-    llm_providers = ["OpenAI", "Anthropic", "OpenRouter", "Ollama"]
+    llm_providers = ["OpenAI", "Anthropic", "OpenRouter", "Ollama", "Gemini"]
     
     selected_llm_provider = st.selectbox(
         "LLM Provider",
@@ -177,7 +178,8 @@
         base_url_help = "Base URL for your LLM provider:\n\n" + \
                         "OpenAI: https://api.openai.com/v1\n\n" + \
                         "Anthropic: https://api.anthropic.com/v1\n\n" + \
-                        "OpenRouter: https://openrouter.ai/api/v1\n\n" + \
+                        "OpenRouter: https://openrouter.ai/api/v1\n\n" +\
+                        "Gemini: No base URL needed (handled internally)\n\n" + \
                         "Ollama: http://localhost:11434/v1"
         
         # Get current BASE_URL or use default for selected provider
@@ -207,6 +209,17 @@
         if selected_llm_provider == "Ollama" and (not current_api_key or profile_env_vars.get("LLM_PROVIDER", "") != selected_llm_provider):
             current_api_key = "NOT_REQUIRED"
         
+        # If provider is Gemini, check for GOOGLE_API_KEY
+        if selected_llm_provider == "Gemini":
+            # Add a note about Google API Key
+            st.info("Gemini models require a Google API key. You can set it directly as LLM_API_KEY or use the separate GOOGLE_API_KEY environment variable.")
+            
+            # Add a specific field for GOOGLE_API_KEY
+            google_api_key = st.text_input(
+                "GOOGLE_API_KEY:",
+                type="password",
+                help="API key for Google Gemini models. Get one at: https://ai.google.dev/tutorials/setup",
+                key="input_GOOGLE_API_KEY",
+                placeholder="Set but hidden" if profile_env_vars.get("GOOGLE_API_KEY") else "")
+            
+            # Only update if user entered something
+            if google_api_key:
+                updated_values["GOOGLE_API_KEY"] = google_api_key
+        
         # If there's already a value, show asterisks in the placeholder
         placeholder = current_api_key if current_api_key == "NOT_REQUIRED" else "Set but hidden" if current_api_key else ""
         api_key = st.text_input(
@@ -323,6 +336,37 @@
         updated_values["SUPABASE_SERVICE_KEY"] = supabase_key if supabase_key else profile_env_vars.get("SUPABASE_SERVICE_KEY", "")
         
         
+        # Advanced Parameters Section
+        st.subheader("Advanced Model Parameters")
+        
+        col1, col2 = st.columns(2)
+        
+        with col1:
+            temperature = st.slider(
+                "Temperature:",
+                min_value=0.0,
+                max_value=1.0,
+                value=float(profile_env_vars.get("MODEL_TEMPERATURE", 0.7)),
+                step=0.05,
+                help="Controls randomness in generation. Lower values are more deterministic, higher values more creative."
+            )
+            updated_values["MODEL_TEMPERATURE"] = str(temperature)
+            
+            max_tokens = st.number_input(
+                "Max Tokens:",
+                min_value=100,
+                max_value=8192,
+                value=int(profile_env_vars.get("MODEL_MAX_TOKENS", 4096)),
+                step=100,
+                help="Maximum number of tokens to generate in responses."
+            )
+            updated_values["MODEL_MAX_TOKENS"] = str(max_tokens)
+        
+        with col2:
+            top_p = st.slider(
+                "Top P:",
+                min_value=0.0,
+                max_value=1.0,
+                value=float(profile_env_vars.get("MODEL_TOP_P", 0.95)),
+                step=0.05,
+                help="Top-p sampling: considers the smallest set of tokens whose cumulative probability exceeds p."
+            )
+            updated_values["MODEL_TOP_P"] = str(top_p)
+            
+            fallback_providers = st.multiselect(
+                "Fallback Order:",
+                options=llm_providers,
+                default=profile_env_vars.get("MODEL_FALLBACK_ORDER", "").split(",") if profile_env_vars.get("MODEL_FALLBACK_ORDER") else [],
+                help="Order of fallback models in case of API failures. Leave empty to disable fallbacks."
+            )
+            updated_values["MODEL_FALLBACK_ORDER"] = ",".join(fallback_providers) if fallback_providers else ""
+        
         # Submit button to save all changes
         st.markdown("---")
         submit_button = st.form_submit_button("Save All Settings")
